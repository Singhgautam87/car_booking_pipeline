pipeline {
    agent any

    environment {
        COMPOSE_FILE = "docker-compose.yml"
        SPARK_SUBMIT = "docker exec spark-master /opt/spark/bin/spark-submit"
        SPARK_PACKAGES = "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2,org.postgresql:postgresql:42.5.0,mysql:mysql-connector-java:8.0.33,org.apache.hadoop:hadoop-aws:3.3.4"
        S3_CONF = "--conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=admin --conf spark.hadoop.fs.s3a.secret.key=admin123 --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
    }
    
    stages {
        stage("Checkout Code") {
            steps {
                checkout scm
                sh "ls -la"
            }
        }
        
        stage('Cleanup') {
            steps {
                sh '''
                    docker-compose down --remove-orphans
                    docker ps -a | grep -E "postgres|mysql|kafka|zookeeper|spark|minio|kafka-ui|great_expectations" | awk '{print $1}' | xargs -r docker rm -f || true
                '''
            }
        }
        
        stage('Start Containers') {
            steps {
                sh '''
                    docker-compose up -d postgres mysql kafka zookeeper minio kafka-ui spark great_expectations
                    echo "⏳ Waiting for Kafka to initialize (60 seconds)..."
                    sleep 60
                    echo "=== Kafka Logs (Last 30 lines) ==="
                    docker logs kafka --tail 30 || echo "Could not fetch Kafka logs"
                    for i in 1 2 3 4 5; do
                        echo "Checking Kafka readiness (attempt $i/5)..."
                        if docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092 2>/dev/null; then
                            echo "✅ Kafka is ready!"
                            break
                        else
                            echo "⏳ Kafka not ready, waiting 10 more seconds..."
                            sleep 10
                        fi
                    done
                    docker-compose ps
                '''
            }
        }

        stage('Kafka Producer') {
            steps {
                sh 'sleep 5'
                sh '''
                    for i in 1 2 3 4 5; do
                        echo "Creating Kafka topic (attempt $i/5)..."
                        if docker exec kafka kafka-topics --create --if-not-exists --topic car-bookings --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1 2>/dev/null; then
                            echo "✅ Topic created successfully!"
                            break
                        else
                            echo "⏳ Retrying topic creation..."
                            sleep 5
                        fi
                    done
                '''
                sh 'docker exec great_expectations pip install kafka-python'
                sh 'docker cp kafka/data/car_booking.json great_expectations:/tmp/car_booking.json'
                sh 'docker cp kafka/producer.py great_expectations:/tmp/producer.py'
                sh 'docker exec great_expectations python /tmp/producer.py'
                sh 'sleep 60'
                sh 'docker exec kafka kafka-topics --describe --topic car-bookings --bootstrap-server localhost:9092'
            }  
        }

        stage('Setup MinIO Bucket') {
            steps {
                sh '''
                    docker exec minio mc alias set local http://localhost:9000 admin admin123
                    docker exec minio mc mb local/raw --ignore-existing
                    docker exec minio mc mb local/processed --ignore-existing
                    docker exec minio mc mb local/curated --ignore-existing
                    echo "✅ Buckets 'raw', 'processed', and 'curated' ready!"
                '''
            }
        }

        stage('Spark Streaming') {
            steps {
                sh 'docker exec -u root spark-master mkdir -p /home/spark/.ivy2/cache'
                sh 'docker exec -u root spark-master chown -R spark:spark /home/spark/.ivy2'
                retry(3) {
                    sh "$SPARK_SUBMIT --packages $SPARK_PACKAGES $S3_CONF /opt/spark-apps/ingest_stream.py"
                }
            }
        }

        stage("Transform & Merge") {
            steps {
                sh "$SPARK_SUBMIT --packages $SPARK_PACKAGES $S3_CONF /opt/spark-apps/transform_booking.py"
                sh "$SPARK_SUBMIT --packages $SPARK_PACKAGES $S3_CONF /opt/spark-apps/transform_customer.py"
                sh "$SPARK_SUBMIT --packages $SPARK_PACKAGES $S3_CONF /opt/spark-apps/merge_customer_booking.py"
                sh "$SPARK_SUBMIT --packages $SPARK_PACKAGES $S3_CONF /opt/spark-apps/write_to_postgres.py"
            }
        }

        stage('Data Quality') {
            steps {
                sh '''
                    docker exec great_expectations python /ge/run_checkpoint.py
                    echo "✅ Data quality checks passed"
                '''
            }
        }

        stage("Load MySQL") {
            steps {
                sh "$SPARK_SUBMIT --packages $SPARK_PACKAGES $S3_CONF /opt/spark-apps/write_to_mysql.py"
            }
        }
    }

    post {
        success {
            echo "✅ Pipeline completed successfully!"
        }
        failure {
            echo "❌ Pipeline failed"
            sh '''
                echo "=== Container Status ==="
                docker-compose ps
                echo ""
                echo "=== Kafka Logs ==="
                docker logs kafka --tail 50 || echo "Kafka logs unavailable"
            '''
        }
    }
}
