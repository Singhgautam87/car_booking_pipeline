pipeline {
    agent any

    environment {
        COMPOSE_FILE = "docker-compose.yml"
    }
    
    stages {
        stage("Checkout Code") {
            steps {
                checkout scm
                sh "ls -la"
            }
        }
        
        stage('Cleanup') {
            steps {
                sh '''
                    # Stop all containers
                    docker-compose down --remove-orphans
            
                    # Clean up any remaining containers
                    docker ps -a | grep -E "postgres|mysql|kafka|zookeeper|spark|minio|kafka-ui|great_expectations" | awk '{print $1}' | xargs -r docker rm -f || true
                '''
            }
        }
        
        stage('Start Containers') {
            steps {
                sh '''
                    # Start all containers
                    docker-compose up -d postgres mysql kafka zookeeper minio kafka-ui spark great_expectations
            
                    echo "‚è≥ Waiting for Kafka to initialize (60 seconds)..."
                    sleep 60
            
                    # Show Kafka logs for debugging
                    echo "=== Kafka Logs (Last 30 lines) ==="
                    docker logs kafka --tail 30 || echo "Could not fetch Kafka logs"
            
                    # Retry Kafka readiness check
                    for i in 1 2 3 4 5; do
                        echo "Checking Kafka readiness (attempt $i/5)..."
                        if docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092 2>/dev/null; then
                            echo "‚úÖ Kafka is ready!"
                            break
                        else
                            echo "‚è≥ Kafka not ready, waiting 10 more seconds..."
                            sleep 10
                        fi
                    done
            
                    # Show container status
                    docker-compose ps
                '''
            }
        }

        stage('Kafka Producer') {
            steps {
                sh 'sleep 5'
                
                // Create topic with retry logic
                sh '''
                    for i in 1 2 3 4 5; do
                        echo "Creating Kafka topic (attempt $i/5)..."
                        if docker exec kafka kafka-topics --create --if-not-exists --topic car-bookings --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1 2>/dev/null; then
                            echo "‚úÖ Topic created successfully!"
                            break
                        else
                            echo "‚è≥ Retrying topic creation..."
                            sleep 5
                        fi
                    done
                '''
                
                sh 'docker exec great_expectations pip install kafka-python'
                sh 'docker cp kafka/data/car_booking.json great_expectations:/tmp/car_booking.json'
                sh 'docker cp kafka/producer.py great_expectations:/tmp/producer.py'
                sh 'docker exec great_expectations python /tmp/producer.py'
                sh 'echo "‚è≥ Waiting for Kafka to stabilize..."'
                sh 'sleep 60'
                sh 'docker exec kafka kafka-topics --describe --topic car-bookings --bootstrap-server localhost:9092'
            }  
        }

        stage('Setup MinIO Bucket') {
            steps {
                sh '''
                    echo "üì¶ Creating MinIO bucket..."
            
                    # MinIO client already available in minio container
                    docker exec minio mc alias set local http://localhost:9000 admin admin123
                    docker exec minio mc mb local/raw --ignore-existing
            
                    echo "‚úÖ Bucket 'raw' ready!"
                '''
            }
        }

        stage('Spark Streaming') {
            steps {
                sh 'docker exec -u root spark-master mkdir -p /home/spark/.ivy2/cache'
                sh 'docker exec -u root spark-master chown -R spark:spark /home/spark/.ivy2'
        
                retry(3) {  
                    sh '''
                        docker exec spark-master /opt/spark/bin/spark-submit \\
                            --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2,org.postgresql:postgresql:42.5.0,mysql:mysql-connector-java:8.0.33,org.apache.hadoop:hadoop-aws:3.3.4 \\
                            --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \\
                            --conf spark.hadoop.fs.s3a.access.key=admin \\
                            --conf spark.hadoop.fs.s3a.secret.key=admin123 \\
                            --conf spark.hadoop.fs.s3a.path.style.access=true \\
                            --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\
                            /opt/spark-apps/ingest_stream.py
                    '''
                }  
            }
        }

        stage("Transform & Merge") {
            steps {
                sh '''
                    docker exec spark-master /opt/spark/bin/spark-submit /opt/spark-apps/transform_booking.py
                    docker exec spark-master /opt/spark/bin/spark-submit /opt/spark-apps/transform_customer.py
                    docker exec spark-master /opt/spark/bin/spark-submit /opt/spark-apps/merge_customer_booking.py
                    docker exec spark-master /opt/spark/bin/spark-submit /opt/spark-apps/write_to_postgres.py

                '''
            }
        }

        stage('Data Quality') {
            steps {
                sh '''
                    docker exec great_expectations python /ge/run_checkpoint.py
                    echo "‚úÖ Data quality checks passed"
                '''
            }
        }

        stage("Load MySQL") {
            steps {
                sh "docker exec spark-master /opt/spark/bin/spark-submit /opt/spark-apps/write_to_mysql.py"
            }
        }
    }

    post {
        success {
            echo "‚úÖ Pipeline completed successfully!"
        }
        failure {
            echo "‚ùå Pipeline failed"
            sh '''
                echo "=== Container Status ==="
                docker-compose ps
                echo ""
                echo "=== Kafka Logs ==="
                docker logs kafka --tail 50 || echo "Kafka logs unavailable"
            '''
        }
    }
}