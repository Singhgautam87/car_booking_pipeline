pipeline {
    agent any

    environment {
        COMPOSE_FILE = "docker-compose.yml"
        COMPOSE_PROJECT = "car-booking-pipeline"
        SPARK_SUBMIT = "docker exec spark-master /opt/spark/bin/spark-submit"
        SPARK_PACKAGES = "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2,org.postgresql:postgresql:42.5.0,mysql:mysql-connector-java:8.0.33,org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-core_2.12:2.4.0"
        DELTA_CONF = "--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
        S3_CONF = "--conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=admin --conf spark.hadoop.fs.s3a.secret.key=admin123 --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
        SPARK_MEMORY = "--driver-memory 2g --executor-memory 2g --conf spark.driver.maxResultSize=1g"
    }

    stages {

        stage('Checkout Code') {
            steps {
                checkout scm
                sh 'ls -la'
            }
        }

        stage('Cleanup') {
            steps {
                sh '''
                    echo "=== Full Cleanup ==="
                    docker rm -f postgres mysql kafka zookeeper minio kafka-ui spark-master great_expectations dashboard || true
                    docker volume rm car-booking-pipeline_mysql_data || true
                    docker volume rm car-booking-pipeline_postgres_data || true
                    docker image prune -f || true
                    rm -rf /tmp/ge_* /tmp/spark_* /tmp/producer.py /tmp/car_booking.json || true
                    echo "‚úÖ Cleanup complete! Jenkins is still running üòé"
                '''
            }
        }

        stage('Start Containers') {
            steps {
                sh '''
                    docker-compose -p ${COMPOSE_PROJECT} up -d \
                        postgres mysql kafka zookeeper minio kafka-ui spark great_expectations

                    echo "Waiting for Kafka to initialize (60 seconds)..."
                    sleep 60

                    echo "=== Kafka Logs ==="
                    docker logs kafka --tail 30 || true

                    for i in 1 2 3 4 5; do
                        echo "Kafka readiness check $i/5..."
                        if docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092 2>/dev/null; then
                            echo "‚úÖ Kafka ready!"
                            break
                        fi
                        sleep 10
                    done

                    docker-compose -p ${COMPOSE_PROJECT} ps

                    # ‚úÖ Spark files copy karo
                    docker cp spark/. spark-master:/opt/spark-apps/
                    echo "‚úÖ Spark files copied to spark-master!"

                    # ‚úÖ Config copy karo spark + GE dono mein
                    docker exec spark-master mkdir -p /opt/config
                    docker cp config/config.json spark-master:/opt/config/config.json
                    echo "‚úÖ Config copied to spark-master!"

                    docker exec great_expectations mkdir -p /opt/config
                    docker cp config/config.json great_expectations:/opt/config/config.json
                    echo "‚úÖ Config copied to great_expectations!"

                    # ‚úÖ Verify
                    echo "--- Config verify ---"
                    docker exec spark-master cat /opt/config/config.json
                '''
            }
        }

        stage('Initialize Databases') {
            steps {
                sh '''
                    echo "Waiting for PostgreSQL..."
                    for i in $(seq 1 30); do
                        if docker exec postgres pg_isready -U admin -d booking >/dev/null 2>&1; then
                            echo "‚úÖ PostgreSQL ready!"
                            break
                        fi
                        sleep 2
                    done

                    echo "Waiting for MySQL..."
                    for i in $(seq 1 30); do
                        if docker exec mysql mysqladmin ping -u admin -padmin >/dev/null 2>&1; then
                            echo "‚úÖ MySQL ready!"
                            break
                        fi
                        sleep 2
                    done

                    echo "Running mysql.sql..."
                    docker exec -i mysql mysql -u admin -padmin booking < sql/mysql.sql

                    echo "Running postgres.sql..."
                    docker exec -i postgres psql -U admin -d booking < sql/postgres.sql

                    echo "‚úÖ All databases ready!"
                '''
            }
        }

        stage('Kafka Producer') {
            steps {
                sh '''
                    sleep 5
                    for i in 1 2 3 4 5; do
                        echo "Creating Kafka topic (attempt $i/5)..."
                        if docker exec kafka kafka-topics --create --if-not-exists \
                            --topic car-bookings \
                            --bootstrap-server localhost:9092 \
                            --partitions 3 \
                            --replication-factor 1 2>/dev/null; then
                            echo "‚úÖ Topic created!"
                            break
                        fi
                        sleep 5
                    done

                    docker exec great_expectations pip install kafka-python --quiet

                    docker cp kafka/data/car_booking.json great_expectations:/tmp/car_booking.json
                    docker cp kafka/producer.py great_expectations:/tmp/producer.py
                    docker exec great_expectations python /tmp/producer.py

                    sleep 60
                    docker exec kafka kafka-topics --describe --topic car-bookings --bootstrap-server localhost:9092
                '''
            }
        }

        stage('Setup MinIO Buckets') {
            steps {
                sh '''
                    docker exec minio sh -c "
                        rm -rf /data/delta &&
                        mkdir -p /data/delta/raw &&
                        mkdir -p /data/delta/transformed &&
                        mkdir -p /data/delta/curated
                    "
                    echo "‚úÖ MinIO buckets ready!"
                '''
            }
        }

        stage('Spark Ingest Stream') {
            steps {
                sh 'docker exec -u root spark-master mkdir -p /home/spark/.ivy2/cache'
                sh 'docker exec -u root spark-master chown -R spark:spark /home/spark/.ivy2'
                retry(3) {
                    timeout(time: 15, unit: 'MINUTES') {
                        sh '''
                            $SPARK_SUBMIT $SPARK_MEMORY --packages $SPARK_PACKAGES $DELTA_CONF $S3_CONF \
                                /opt/spark-apps/jobs/ingest_stream.py
                            echo "‚úÖ Raw data Delta Lake mein!"
                        '''
                    }
                }
            }
        }

        stage('Transform Data') {
            steps {
                timeout(time: 20, unit: 'MINUTES') {
                    sh '''
                        $SPARK_SUBMIT $SPARK_MEMORY --packages $SPARK_PACKAGES $DELTA_CONF $S3_CONF \
                            /opt/spark-apps/jobs/transform_customer.py
                        echo "‚úÖ Customer done"

                        $SPARK_SUBMIT $SPARK_MEMORY --packages $SPARK_PACKAGES $DELTA_CONF $S3_CONF \
                            /opt/spark-apps/jobs/transform_booking.py
                        echo "‚úÖ Booking done"

                        $SPARK_SUBMIT $SPARK_MEMORY --packages $SPARK_PACKAGES $DELTA_CONF $S3_CONF \
                            /opt/spark-apps/jobs/transform_cars.py
                        echo "‚úÖ Cars done"

                        $SPARK_SUBMIT $SPARK_MEMORY --packages $SPARK_PACKAGES $DELTA_CONF $S3_CONF \
                            /opt/spark-apps/jobs/transform_payments.py
                        echo "‚úÖ Payments done"
                    '''
                }
            }
        }

        stage('Merge Data') {
            steps {
                timeout(time: 15, unit: 'MINUTES') {
                    sh '''
                        $SPARK_SUBMIT $SPARK_MEMORY --packages $SPARK_PACKAGES $DELTA_CONF $S3_CONF \
                            /opt/spark-apps/jobs/merge.py
                        echo "‚úÖ Data merged!"
                    '''
                }
            }
        }

        stage('Write to PostgreSQL Staging') {
            steps {
                timeout(time: 15, unit: 'MINUTES') {
                    sh '''
                        $SPARK_SUBMIT $SPARK_MEMORY --packages $SPARK_PACKAGES $DELTA_CONF $S3_CONF \
                            /opt/spark-apps/jobs/write_to_postgres.py
                        echo "‚úÖ PostgreSQL staging ready!"
                    '''
                }
            }
        }

        stage('Data Validation') {
            steps {
                sh '''
                    echo "=== Data Validation Setup ==="

                    docker exec great_expectations mkdir -p /ge/jobs
                    docker exec great_expectations mkdir -p /ge/great_expectations/expectations
                    docker exec great_expectations mkdir -p /ge/great_expectations/uncommitted/validations

                    docker cp great_expectations/jobs/run_ge.py great_expectations:/ge/jobs/run_ge.py
                    docker cp great_expectations/great_expectations/expectations/customer_booking_suite.json \
                        great_expectations:/ge/great_expectations/expectations/customer_booking_suite.json

                    docker exec mysql mysql -u admin -padmin booking -e "
                        DELETE FROM validation_results;
                        DELETE FROM validation_expectation_details;
                        DELETE FROM validation_failed_records;
                    "

                    echo "Running validation..."
                    docker exec great_expectations python /ge/jobs/run_ge.py
                    echo "‚úÖ Validation complete!"
                '''
            }
        }

        stage('Load MySQL') {
            steps {
                sh 'docker exec -u root spark-master pip install mysql-connector-python --quiet'
                timeout(time: 15, unit: 'MINUTES') {
                    sh '''
                        $SPARK_SUBMIT $SPARK_MEMORY --packages $SPARK_PACKAGES $DELTA_CONF $S3_CONF \
                            /opt/spark-apps/jobs/write_to_mysql.py
                        echo "‚úÖ MySQL load complete!"
                    '''
                }
            }
        }

        stage('Start Dashboard') {
            steps {
                sh '''
                    docker rm -f dashboard || true

                    docker-compose -p ${COMPOSE_PROJECT} up -d dashboard

                    sleep 15

                    for i in 1 2 3 4 5; do
                        if curl -s http://localhost:8050 >/dev/null 2>&1; then
                            echo "‚úÖ Dashboard running at http://localhost:8050"
                            break
                        fi
                        echo "Waiting for dashboard... $i/5"
                        sleep 10
                    done
                '''
            }
        }
    }

    post {
        success {
            echo "‚úÖ Pipeline complete!"
            echo "üìä Dashboard: http://localhost:8050"
            echo "üìã Kafka UI: http://localhost:8081"
            echo "üóÑÔ∏è MinIO: http://localhost:9001"
            echo "‚ö° Spark UI: http://localhost:8082"
        }
        failure {
            sh '''
                echo "=== Container Status ==="
                docker-compose -p ${COMPOSE_PROJECT} ps || true
                echo "=== Kafka Logs ==="
                docker logs kafka --tail 50 || true
                echo "=== Spark Logs ==="
                docker logs spark-master --tail 50 || true
                echo "=== Validation Logs ==="
                docker logs great_expectations --tail 50 || true
                echo "=== MySQL Logs ==="
                docker logs mysql --tail 30 || true
                echo "=== PostgreSQL Logs ==="
                docker logs postgres --tail 30 || true
                echo "=== Dashboard Logs ==="
                docker logs dashboard --tail 30 || true
            '''
            echo "‚ùå Pipeline failed!"
        }
    }
}
